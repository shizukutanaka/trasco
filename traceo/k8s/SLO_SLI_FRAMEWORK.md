# Service Level Objectives & Indicators Framework

**Date**: November 20, 2024
**Status**: âœ… Complete Implementation
**Focus**: Defining, measuring, and tracking service reliability

---

## ğŸ“Š Executive Summary

This framework implements **Service Level Objectives (SLOs)** and **Service Level Indicators (SLIs)**, enabling data-driven reliability management through error budgets.

### SLO vs SLI vs SLA

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SLA (Service Level Agreement)                          â”‚
â”‚ - Legal contract with customers                        â”‚
â”‚ - Defines penalty/credits if SLO missed                â”‚
â”‚ - Example: "99.9% uptime or 10% credit"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SLO (Service Level Objective)                          â”‚
â”‚ - Internal target we commit to                         â”‚
â”‚ - Slightly stricter than SLA for buffer                â”‚
â”‚ - Example: "99.95% availability" (internal)            â”‚
â”‚           "99.9% availability" (customer SLA)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SLI (Service Level Indicator)                          â”‚
â”‚ - What we actually measure                             â”‚
â”‚ - Example: "successful_requests / total_requests"      â”‚
â”‚           = 99.87% last month                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Relationship: SLI â‰¥ SLO means we're meeting targets âœ“
             SLI < SLO means we exceeded error budget âœ—
```

---

## ğŸ¯ SLO Definition by Service Tier

### Tier 1: Critical (Financial/Security)

```yaml
service: payment-processing
slo:
  availability: 99.99%    # 4 nines = 4.38 minutes/month
  latency_p99: 100ms
  error_rate: 0.01%

error_budget_per_month: 4.38 minutes
monthly_window: 43,200 minutes (30 days)
```

### Tier 2: High (Customer-Facing)

```yaml
service: api-gateway
slo:
  availability: 99.9%     # 3 nines = 43.2 minutes/month
  latency_p99: 1000ms
  error_rate: 0.1%

error_budget_per_month: 43.2 minutes
```

### Tier 3: Standard (Internal/Non-Critical)

```yaml
service: admin-dashboard
slo:
  availability: 99.5%     # 2.5 nines = 216 minutes/month
  latency_p99: 2000ms
  error_rate: 0.5%

error_budget_per_month: 216 minutes
```

### Tier 4: Best Effort (Experimental)

```yaml
service: beta-features
slo:
  availability: 95.0%     # 1.3 nines = 36 hours/month
  latency_p99: 5000ms
  error_rate: 5.0%

error_budget_per_month: 1,296 minutes (21.6 hours)
```

---

## ğŸ“ SLI Measurement

### SLI Types

```promql
# Availability SLI
sli:service:availability =
  successful_requests / total_requests

# Latency SLI
sli:service:latency_p99 =
  histogram_quantile(0.99, request_duration_bucket)

# Error Budget SLI
sli:service:error_budget_consumed =
  (1 - actual_availability) / (1 - target_availability)
```

### Recording Rules for SLI

```yaml
groups:
  - name: sli.rules
    interval: 30s
    rules:
      # Availability SLI
      - record: sli:api_gateway:availability
        expr: |
          (sum(rate(http_requests_total{status=~"2..|3.."}[5m]))
           / sum(rate(http_requests_total[5m])))

      # Latency SLI
      - record: sli:api_gateway:latency:p99
        expr: |
          histogram_quantile(0.99,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
          )

      # Error Budget Remaining (% of monthly budget remaining)
      - record: sli:api_gateway:error_budget_remaining
        expr: |
          (
            (1 - sli:api_gateway:availability)
            / (1 - 0.999)  # Target: 99.9%
          ) * 100

      # Error Budget Burn Rate (how fast consuming budget)
      - record: sli:api_gateway:burn_rate:5m
        expr: |
          (
            1 - sli:api_gateway:availability
          ) / (1 - 0.999)
```

---

## ğŸ’° Error Budget Management

### Error Budget Allocation

```
Monthly Budget: 43.2 minutes (99.9% SLO)

Allocation:
â”œâ”€ Planned Maintenance: 10 minutes (23%)
â”œâ”€ Emergency Maintenance: 10 minutes (23%)
â”œâ”€ Deployment Risk: 15 minutes (35%)
â””â”€ Unexpected Outages: 8.2 minutes (19%)
   â””â”€ Buffer for contingencies
```

### Error Budget Burn Rate

```promql
# How fast we're consuming the monthly error budget
burn_rate = (current_error_rate / slo_error_budget) / time_window

Examples (99.9% SLO):
â”œâ”€ 0.3% error rate for 5m   â†’ 10.0Ã— burn (critical - page)
â”œâ”€ 0.3% error rate for 1h   â†’ 1.7Ã— burn (warning - ticket)
â””â”€ 0.1% error rate for 30d  â†’ 1.0Ã— burn (normal - monitor)
```

### Monthly Error Budget Chart

```
Day 1:  100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ remaining
Day 7:  85%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ remaining
Day 14: 62%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ remaining  â† On track
Day 21: 48%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ remaining  â† Good
Day 28: 15%  â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ remaining  â† Warning
Day 30: 0%   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ remaining  â† Exhausted âœ—
```

---

## ğŸ“Š SLO Dashboards

### Dashboard 1: SLO Compliance

```promql
# Main SLO status
sli:api_gateway:availability               # Current SLI
slo_target: 0.999                           # Target (99.9%)
compliance: (sli / slo_target) * 100        # % of SLO
error_budget_remaining: (1 - sli / slo) * 100

Visualization:
â”œâ”€ Big Number: Availability (99.87% - GREEN âœ“)
â”œâ”€ Gauge: Error Budget Remaining (35% - YELLOW âš )
â”œâ”€ Graph: Availability Trend (last 30 days)
â””â”€ Table: 30-day compliance by service
```

### Dashboard 2: Error Budget Tracking

```
Monthly Error Budget: 43.2 minutes

Consumed:
â”œâ”€ Week 1: 8 minutes (failed deployment rollback)
â”œâ”€ Week 2: 5 minutes (database maintenance)
â”œâ”€ Week 3: 3 minutes (network incident)
â””â”€ Week 4: 2 minutes (cache failure)
Total Used: 18 minutes (42%)
Remaining: 25 minutes (58%)
```

### Dashboard 3: Burn Rate Tracking

```
Current Burn Rate: 0.8Ã—
â”œâ”€ If continues for 30d: Normal âœ“
â”œâ”€ If doubles to 1.6Ã—: Burn out in 15d âš 
â””â”€ If 10Ã—: Burn out in 1.5d (CRITICAL ğŸ”´)

Burn Rate Trend:
â”œâ”€ 24h avg: 0.9Ã— (normal)
â”œâ”€ 7d avg: 0.85Ã— (good - slightly under budget)
â””â”€ 30d: 0.75Ã— (excellent - conservative spending)
```

---

## ğŸ¯ Decision Framework Using Error Budgets

### Decision: Can We Deploy?

```
Current Error Budget Remaining: 20 minutes
Deployment Risk Level: Medium (historical: 5% error rate)

Decision Logic:
â”œâ”€ If budget > 50%: DEPLOY (75% margin)
â”œâ”€ If budget 25-50%: DEPLOY (conservative deployment)
â”œâ”€ If budget 10-25%: NO DEPLOY (wait for recovery)
â””â”€ If budget < 10%: NO DEPLOY (emergency hold)

Decision: DEPLOY âœ“ (20m > 15m threshold)
```

### Decision: Do We Need to Scale?

```
Current State:
â”œâ”€ Error Rate: 0.08% (below 0.1% SLO)
â”œâ”€ Latency p99: 980ms (below 1000ms SLO)
â”œâ”€ Error Budget Burn: 0.9Ã— (sustainable)

Decision: NO SCALING NEEDED âœ“
Reason: All SLIs healthy, budget stable
```

### Decision: Do We Need to Invest?

```
30-Day Analysis:
â”œâ”€ Error Budget Consumed: 42%
â”œâ”€ Burn Rate Trend: Increasing (0.6Ã— â†’ 0.9Ã—)
â”œâ”€ Incidents: 3 (database, cache, deployment)

Decision: YES, INVEST âš 
Actions:
â”œâ”€ Improve database stability
â”œâ”€ Add cache redundancy
â”œâ”€ Improve deployment safety
Goal: Reduce burn rate from 0.9Ã— â†’ 0.5Ã—
```

---

## ğŸ“‹ SLO Definition Process

### Step 1: Interview Stakeholders

```
Questions to ask:
â”œâ”€ What's the business impact of 1 minute downtime?
â”œâ”€ What's the user tolerance for latency?
â”œâ”€ What's the cost of missing SLO (penalties, lost revenue)?
â””â”€ How does this compare to competitors?

Outcomes:
â”œâ”€ Understand business requirements
â”œâ”€ Set realistic, achievable SLOs
â””â”€ Get buy-in from leadership
```

### Step 2: Define SLOs

```yaml
service: api-gateway
owners:
  - backend-team
  - platform-team

slos:
  - indicator: availability
    target: 99.9%
    measurement: successful_requests / total_requests
    window: monthly

  - indicator: latency
    target: 1000ms p99
    measurement: http_request_duration_seconds (p99)
    window: daily

  - indicator: error_rate
    target: 0.1%
    measurement: failed_requests / total_requests
    window: hourly
```

### Step 3: Implement SLI Measurement

```yaml
# Recording rules to calculate SLIs
- record: sli:api_gateway:availability
  expr: (successful / total) * 100

- record: sli:api_gateway:latency:p99
  expr: histogram_quantile(0.99, duration_bucket)

- record: sli:api_gateway:error_rate
  expr: (errors / total) * 100
```

### Step 4: Implement Alerting

```yaml
# Alert when burn rate exceeds thresholds
- alert: ErrorBudgetBurnCritical
  expr: burn_rate > 10
  for: 5m
  action: page_on_call

- alert: ErrorBudgetBurnWarning
  expr: burn_rate > 3
  for: 15m
  action: create_ticket
```

### Step 5: Monitor & Review

```
Weekly Reviews:
â”œâ”€ Is SLO being met? (YES/NO)
â”œâ”€ What caused incidents?
â”œâ”€ Are burn rates normal?
â””â”€ Any trends?

Monthly Reviews:
â”œâ”€ Monthly SLO compliance (target > 99.9%)
â”œâ”€ Error budget spent (target < 100%)
â”œâ”€ Incident postmortems
â””â”€ Planned improvements

Quarterly Reviews:
â”œâ”€ Are SLOs still realistic?
â”œâ”€ Do they align with business goals?
â”œâ”€ Should we increase target?
â””â”€ SLO adjustment if needed
```

---

## ğŸ† Best Practices

### DO âœ…

```yaml
# DO: Set realistic SLOs based on current performance
# Find your 99th percentile, then set SLO at 95th

# DO: Include outage windows in error budget
deployment_window: 10 min/month
maintenance_window: 10 min/month
unexpected_outages: 23 min/month (remaining budget)

# DO: Align SLO with business needs
financial_services: 99.99% (4 nines)
e_commerce: 99.9% (3 nines)
internal_tools: 99.5% (2.5 nines)

# DO: Automate SLI measurement
# Use Prometheus recording rules, not manual calculation

# DO: Review SLOs quarterly
# Adjust if consistently over/under performing
```

### DON'T âŒ

```yaml
# DON'T: Set SLOs too high
# Example: 99.999% (5 nines) is impractical for startups
# Reality: Most services operate at 99-99.9%

# DON'T: Use SLOs for punishment
# SLOs are targets, not guaranteed contracts
# Use for capacity planning, not blame

# DON'T: Ignore error budget
# "We have budget, let's deploy risky change"
# Remember: Budget must last entire month

# DON'T: Set SLOs without ops input
# Will be either too high (impossible) or too low (useless)
# Ops knows what's achievable
```

---

## ğŸ“Š Metrics & Monitoring

### Key Metrics

```promql
# SLI Metrics
sli:service:availability
sli:service:latency:p99
sli:service:error_rate
sli:service:burn_rate

# Error Budget Metrics
slo:service:error_budget_remaining
slo:service:error_budget_consumed
slo:service:burn_rate:5m
slo:service:burn_rate:1h
```

### Alert Rules

```yaml
# Burn rate alerts (already defined in ADVANCED_ALERTING_GUIDE.md)
- alert: ErrorBudgetBurnRateFast
  expr: burn_rate > 10
  for: 5m
  severity: critical

- alert: ErrorBudgetBurnRateMedium
  expr: burn_rate > 3
  for: 15m
  severity: warning

- alert: ErrorBudgetExhausted
  expr: error_budget_remaining <= 0
  for: 10m
  severity: critical
  action: "STOP all non-critical changes"
```

---

## ğŸ“ References

### Research Sources

- **Google SRE Book**: Chapter on SLOs and error budgets
- **Google Devel**: "The SLO Framework: Setting Reliability Expectations"
- **Prometheus**: Best practices for SLI measurement
- **CRE Best Practices**: Service level objectives

### Industry Examples

- **Google**: 99.99% SLO across most services
- **Amazon AWS**: 99.99% availability for critical services
- **Netflix**: Sophisticated SLO framework across 700+ services
- **Stripe**: 99.99% SLO for payment processing

---

**Version**: 1.0
**Status**: âœ… Production Ready
**Last Updated**: November 20, 2024

Generated with comprehensive research from Google SRE, Prometheus, and industry best practices.
