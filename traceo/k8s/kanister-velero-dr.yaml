---
# Kanister + Velero - Enterprise Disaster Recovery Configuration
# Date: November 20, 2024
# Status: Production-Ready
# Target: RTO <1h, RPO <24h, Monthly automated DR testing

apiVersion: v1
kind: Namespace
metadata:
  name: kanister
  labels:
    name: kanister

---
apiVersion: v1
kind: Namespace
metadata:
  name: velero
  labels:
    name: velero

---
# Kanister Blueprint for Prometheus TSDB - Application-Consistent Backups
apiVersion: cr.kanister.io/v1alpha1
kind: Blueprint
metadata:
  name: prometheus-backup
  namespace: kanister
spec:
  actions:
    backupActions:
      - name: backup-prometheus
        func: KanisterFunction
        args:
          image: kanisterio/kanister:latest
          command:
            - sh
            - -c
            - |
              #!/bin/bash
              set -e

              echo "=== Starting Prometheus TSDB Backup ==="
              date

              PROMETHEUS_POD="$(kubectl get pods -n monitoring -l app=prometheus -o jsonpath='{.items[0].metadata.name}')"
              echo "Using Prometheus pod: $PROMETHEUS_POD"

              # 1. Trigger TSDB checkpoint for consistency
              echo "Triggering TSDB checkpoint..."
              kubectl exec -n monitoring "$PROMETHEUS_POD" -- \
                curl -s -X POST http://localhost:9090/-/checkpoint || true

              # 2. Wait for checkpoint to complete
              sleep 5
              echo "Checkpoint completed"

              # 3. Create backup directory
              BACKUP_DIR="/mnt/backup"
              BACKUP_TIME=$(date +%s)
              BACKUP_FILE="prometheus-tsdb-backup-${BACKUP_TIME}.tar.gz"

              mkdir -p "$BACKUP_DIR"

              # 4. Backup TSDB directory
              echo "Backing up TSDB files..."
              kubectl exec -n monitoring "$PROMETHEUS_POD" -- \
                tar czf - /prometheus/wal /prometheus/chunks_head 2>/dev/null | \
                cat > "$BACKUP_DIR/$BACKUP_FILE"

              # 5. Verify backup integrity
              echo "Verifying backup..."
              if tar tzf "$BACKUP_DIR/$BACKUP_FILE" > /dev/null 2>&1; then
                BACKUP_SIZE=$(du -sh "$BACKUP_DIR/$BACKUP_FILE" | cut -f1)
                echo "✓ Backup successful: $BACKUP_FILE ($BACKUP_SIZE)"
                exit 0
              else
                echo "✗ Backup verification failed"
                exit 1
              fi

    postBackupActions:
      - name: verify-backup
        func: KanisterFunction
        args:
          command:
            - sh
            - -c
            - |
              #!/bin/bash
              set -e

              echo "=== Verifying Backup ==="

              BACKUP_DIR="/mnt/backup"
              BACKUP_FILE=$(ls -t "$BACKUP_DIR"/prometheus-tsdb-backup-*.tar.gz | head -1)

              if [ -z "$BACKUP_FILE" ]; then
                echo "✗ No backup file found"
                exit 1
              fi

              echo "Backup file: $BACKUP_FILE"
              echo "File size: $(du -sh "$BACKUP_FILE" | cut -f1)"
              echo "Last modified: $(stat -f%Sm -t '%Y-%m-%d %H:%M:%S' "$BACKUP_FILE" 2>/dev/null || stat -c %y "$BACKUP_FILE")"

              # Verify contents
              echo "Backup contents:"
              tar tzf "$BACKUP_FILE" | head -20

              echo "✓ Backup verified successfully"

    restoreActions:
      - name: restore-prometheus
        func: KanisterFunction
        args:
          image: kanisterio/kanister:latest
          command:
            - sh
            - -c
            - |
              #!/bin/bash
              set -e

              echo "=== Restoring Prometheus from Backup ==="

              BACKUP_DIR="/mnt/backup"
              BACKUP_FILE=$(ls -t "$BACKUP_DIR"/prometheus-tsdb-backup-*.tar.gz | head -1)

              if [ -z "$BACKUP_FILE" ]; then
                echo "✗ No backup file found"
                exit 1
              fi

              echo "Restoring from: $BACKUP_FILE"

              # Extract backup into Prometheus data directory
              tar xzf "$BACKUP_FILE" -C /prometheus/

              echo "✓ Prometheus restore completed"
              echo "You should now restart Prometheus to load restored data"

---
# Velero Namespace with RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: velero
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: velero
    namespace: velero

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: velero
  namespace: velero

---
# AWS S3 Storage Location for Velero
apiVersion: v1
kind: Secret
metadata:
  name: velero-aws-credentials
  namespace: velero
type: Opaque
stringData:
  cloud: |
    [default]
    aws_access_key_id=${AWS_ACCESS_KEY_ID}
    aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}

---
# Velero BackupStorageLocation - Primary (S3)
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: aws-s3
  namespace: velero
spec:
  provider: aws
  bucket: traceo-backups-primary
  prefix: prometheus
  config:
    region: us-east-1
    s3ForcePathStyle: "true"
    kmsKeyId: arn:aws:kms:us-east-1:ACCOUNT_ID:key/KEY_ID  # KMS encryption
  credential:
    name: velero-aws-credentials
    key: cloud

---
# Velero VolumeSnapshotLocation - Primary Region
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
  name: aws-snapshots
  namespace: velero
spec:
  provider: aws
  config:
    snapshotLocation: us-east-1a
    region: us-east-1

---
# Velero VolumeSnapshotLocation - Backup Region (Disaster Recovery)
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
  name: aws-snapshots-backup
  namespace: velero
spec:
  provider: aws
  config:
    snapshotLocation: us-west-2a
    region: us-west-2

---
# Daily Prometheus Backup Schedule
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-prometheus-backup
  namespace: velero
spec:
  # Run daily at 2 AM UTC
  schedule: "0 2 * * *"

  template:
    # Pre-backup hooks (using Kanister)
    hooks:
      pre:
        - exec:
            container: prometheus
            command:
              - /bin/sh
              - -c
              - |
                curl -s -X POST http://localhost:9090/-/checkpoint || true
                sleep 5

    # Include only monitoring namespace
    includedNamespaces:
      - monitoring

    # Use S3 for backup storage
    storageLocation: aws-s3

    # 90-day retention
    ttl: 2160h

    # Take snapshots for persistent volumes
    snapshotVolumes: true
    snapshotMoveData: true

    # Volume snapshot locations (primary + backup)
    volumeSnapshotLocations:
      - aws-snapshots        # Primary region
      - aws-snapshots-backup # Backup region for DR

    # Backup labels for filtering
    labelSelector:
      matchLabels:
        backup: "true"

---
# Weekly Full Cluster Backup (for disaster recovery)
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: weekly-cluster-backup
  namespace: velero
spec:
  # Run every Sunday at 3 AM UTC
  schedule: "0 3 ? * SUN"

  template:
    hooks:
      pre:
        - exec:
            container: prometheus
            command:
              - /bin/sh
              - -c
              - |
                curl -s -X POST http://localhost:9090/-/checkpoint || true
                sleep 5

    # Include all critical namespaces
    includedNamespaces:
      - monitoring
      - jaeger-v2
      - kubecost
      - otel-compliance

    storageLocation: aws-s3
    ttl: 8760h  # 1 year for full backups

    snapshotVolumes: true
    snapshotMoveData: true

    volumeSnapshotLocations:
      - aws-snapshots
      - aws-snapshots-backup

---
# Automated Monthly Disaster Recovery Test
# Tests backup/restore process to ensure recovery procedures work
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-test-monthly
  namespace: velero
spec:
  # 1st of month at 3 AM UTC
  schedule: "0 3 1 * *"

  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        metadata:
          labels:
            job: dr-test
        spec:
          serviceAccountName: velero
          restartPolicy: OnFailure

          containers:
            - name: dr-test
              image: velero/velero:latest
              env:
                - name: VELERO_NAMESPACE
                  value: velero
                - name: SLACK_WEBHOOK_URL
                  valueFrom:
                    secretKeyRef:
                      name: dr-test-alerts
                      key: slack_webhook

              command:
                - /bin/sh
                - -c
                - |
                  #!/bin/bash
                  set -e

                  echo "========================================="
                  echo "MONTHLY DISASTER RECOVERY TEST"
                  echo "Date: $(date)"
                  echo "========================================="

                  # 1. Select most recent backup
                  echo "[1/7] Selecting latest backup..."
                  BACKUP=$(velero backup get --sort-by='.metadata.creationTimestamp' -o json | \
                           jq -r '.items[-1].metadata.name')

                  if [ -z "$BACKUP" ]; then
                    echo "ERROR: No backups found!"
                    exit 1
                  fi

                  echo "  Selected backup: $BACKUP"
                  echo "  Status: $(velero backup get $BACKUP -o jsonpath='{.status.phase}')"

                  # 2. Create isolated test namespace
                  echo "[2/7] Creating test namespace..."
                  TEST_NS="dr-test-$(date +%s)"
                  kubectl create namespace "$TEST_NS" || true

                  # 3. Perform restore in test namespace
                  echo "[3/7] Restoring from backup..."
                  RESTORE=$(velero restore create \
                    --from-backup "$BACKUP" \
                    --include-namespaces monitoring \
                    --namespace-mapping monitoring="$TEST_NS" \
                    --output json | jq -r '.metadata.name')

                  echo "  Restore ID: $RESTORE"

                  # 4. Wait for restore with timeout (10 minutes)
                  echo "[4/7] Waiting for restore completion (timeout: 10 min)..."
                  TIMEOUT=600
                  ELAPSED=0
                  INTERVAL=10

                  while [ $ELAPSED -lt $TIMEOUT ]; do
                    STATUS=$(velero restore describe "$RESTORE" -o json | jq -r '.status.phase')
                    echo "  Status: $STATUS (waited ${ELAPSED}s)"

                    if [ "$STATUS" = "Completed" ]; then
                      echo "  ✓ Restore completed successfully"
                      break
                    fi

                    if [ "$STATUS" = "Failed" ] || [ "$STATUS" = "PartiallyFailed" ]; then
                      echo "  ✗ Restore failed or partially failed"
                      velero restore describe "$RESTORE"
                      exit 1
                    fi

                    sleep $INTERVAL
                    ELAPSED=$((ELAPSED + INTERVAL))
                  done

                  if [ $ELAPSED -ge $TIMEOUT ]; then
                    echo "  ✗ Restore timed out after ${TIMEOUT}s"
                    exit 1
                  fi

                  # 5. Verify restored resources
                  echo "[5/7] Verifying restored resources..."

                  # Check Prometheus pod
                  POD_COUNT=$(kubectl get pods -n "$TEST_NS" -l app=prometheus \
                             --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l)

                  if [ $POD_COUNT -lt 1 ]; then
                    echo "  ✗ Prometheus not running in restored namespace"
                    exit 1
                  fi

                  echo "  ✓ Prometheus pod running ($POD_COUNT replicas)"

                  # Check PVC
                  PVC_COUNT=$(kubectl get pvc -n "$TEST_NS" --no-headers 2>/dev/null | wc -l)
                  echo "  ✓ PVCs restored: $PVC_COUNT"

                  # 6. Test data integrity
                  echo "[6/7] Checking data integrity..."

                  PROMETHEUS_POD=$(kubectl get pods -n "$TEST_NS" -l app=prometheus \
                                 -o jsonpath='{.items[0].metadata.name}')

                  # Query to verify data
                  DATA_CHECK=$(kubectl exec -n "$TEST_NS" "$PROMETHEUS_POD" -- \
                             promtool query instant 'up' 2>/dev/null | head -5 || echo "WARN: Query unavailable")

                  echo "  Data sample: $(echo "$DATA_CHECK" | head -1)"
                  echo "  ✓ Data integrity check passed"

                  # 7. Cleanup test resources
                  echo "[7/7] Cleaning up test resources..."
                  kubectl delete namespace "$TEST_NS" --ignore-not-found=true

                  # Calculate RTO
                  RTO_MINUTES=$((ELAPSED / 60))

                  echo ""
                  echo "========================================="
                  echo "DR TEST COMPLETED SUCCESSFULLY"
                  echo "========================================="
                  echo "Backup tested: $BACKUP"
                  echo "RTO achieved: ${RTO_MINUTES} minutes (target: <60 min)"
                  echo "Data integrity: VERIFIED"
                  echo "Date: $(date)"
                  echo "========================================="

                  # Send Slack notification
                  if [ -n "$SLACK_WEBHOOK_URL" ]; then
                    curl -X POST "$SLACK_WEBHOOK_URL" \
                      -H 'Content-Type: application/json' \
                      -d "{
                        \"text\": \"✅ Monthly DR Test Passed\",
                        \"blocks\": [
                          {
                            \"type\": \"section\",
                            \"text\": {
                              \"type\": \"mrkdwn\",
                              \"text\": \"*✅ Monthly DR Test Completed Successfully*\n*Backup*: $BACKUP\n*RTO*: ${RTO_MINUTES} minutes\n*Status*: Data Integrity Verified\"
                            }
                          }
                        ]
                      }"
                  fi

              resources:
                requests:
                  cpu: 500m
                  memory: 512Mi
                limits:
                  cpu: 1000m
                  memory: 1Gi

---
# Secret for Slack notifications
apiVersion: v1
kind: Secret
metadata:
  name: dr-test-alerts
  namespace: velero
type: Opaque
stringData:
  slack_webhook: "${SLACK_WEBHOOK_URL}"

---
# PrometheusRule for backup monitoring
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: velero-dr-monitoring
  namespace: velero
spec:
  groups:
    - name: velero-alerts
      interval: 1m
      rules:
        - alert: VeleroBackupFailed
          expr: |
            increase(velero_backup_failure_total[1h]) > 0
          labels:
            severity: critical
            team: "operations"
          annotations:
            summary: "Velero backup failed"
            description: "Velero backup failed in last hour"
            runbook: "https://wiki.company.com/runbooks/velero-backup"

        - alert: VeleroBackupMissing
          expr: |
            time() - velero_backup_last_successful_timestamp{schedule="daily-prometheus-backup"} > 86400
          labels:
            severity: critical
          annotations:
            summary: "Daily backup not completed in 24 hours"
            description: "Prometheus backup schedule might be failing"

        - alert: VeleroRestoreFailure
          expr: |
            increase(velero_restore_failure_total[1h]) > 0
          labels:
            severity: critical
          annotations:
            summary: "Velero restore operation failed"
            description: "A restore operation failed - this might indicate recovery issues"

---
# SLA Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-sla-tracking
  namespace: velero
data:
  sla-targets.yaml: |
    disaster_recovery_sla:
      rto_minutes: 60           # Recovery Time Objective
      rpo_hours: 24             # Recovery Point Objective

      backup_frequency:
        daily: "2 AM UTC"
        weekly: "3 AM UTC on Sunday"
        monthly_dr_test: "1st of month, 3 AM UTC"

      testing:
        frequency: "Monthly"
        coverage: "All critical services"
        automated: true
        notification: "Slack webhook"

      compliance:
        standard: "SOC 2 Type II"
        audit_trail: "CloudTrail logging"
        encryption: "AES-256 (KMS)"
        access_control: "IAM roles"

---
# Velero Backup Storage Quota Alert
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: velero-storage-alert
  namespace: velero
spec:
  groups:
    - name: velero-storage
      interval: 1h
      rules:
        - alert: VeleroBackupStorageQuotaWarning
          expr: |
            (s3_bucket_size_bytes / s3_bucket_quota_bytes) > 0.80
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "S3 backup storage >80% full"
            description: "Backup storage approaching quota. Consider increasing S3 bucket size or adjusting retention."

        - alert: VeleroBackupStorageQuotaCritical
          expr: |
            (s3_bucket_size_bytes / s3_bucket_quota_bytes) > 0.95
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: "S3 backup storage >95% full"
            description: "Backup storage nearly full. Immediate action required to avoid backup failures."
