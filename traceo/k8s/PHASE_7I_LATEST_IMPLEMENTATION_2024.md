# Phase 7I: Latest Implementation Guide (2024 Best Practices)

**Date**: November 20, 2024
**Status**: ğŸš€ Production-Ready Implementation (Latest 2024 Practices)
**Research**: Multilingual sources, CNCF, academic papers, industry case studies

---

## ğŸ¯ Week 1: Latest Trace Sampling Implementation

### 1. Jaeger v2 + Adaptive Sampling (November 2024)

**Jaeger v2 Key Improvements**:
- OpenTelemetry Collector at core
- Adaptive sampling (ML-driven from TailCtrl acquisition)
- 40-60% storage cost reduction
- Full OpenTelemetry compatibility

#### Deployment Configuration

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: jaeger-v2

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: jaeger-collector-config
  namespace: jaeger-v2
data:
  otel-collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250

    processors:
      # Head-based sampling: Sample 100% errors + high-latency
      sampling:
        sampling_percentage: 5  # 5% baseline + special cases
        policies:
          - name: "error-traces"
            type: "status_code"
            status_code:
              status_codes: [ERROR, UNSET]
              fraction_on_match: 1.0  # 100% of errors

          - name: "high-latency"
            type: "latency"
            latency:
              threshold_ms: 1000
              fraction_on_match: 0.25  # 25% of slow traces

          - name: "default"
            type: "probabilistic"
            probabilistic:
              sampling_percentage: 5  # 5% baseline

      # Resource detection
      resource_detection:
        detectors: [gcp, aws, azure, system, docker, kubernetes, env]
        timeout: 2s
        override: true

      # Tail-based sampling (for Tempo backend)
      tail_sampling:
        policies:
          - name: error-traces
            type: status_code
            status_code:
              status_codes: [ERROR]
              fraction_on_match: 1.0

          - name: slow-traces
            type: latency
            latency:
              threshold_ms: 1000
              fraction_on_match: 0.1

          - name: alphabetical-service
            type: string_attribute
            string_attribute:
              key: service.name
              values: [api, auth, payment]
              fraction_on_match: 0.5

      batch:
        send_batch_size: 1024
        timeout: 10s

    exporters:
      jaeger:
        endpoint: jaeger-collector.jaeger-v2:14250
        tls:
          insecure: false

      otlp:
        endpoint: tempo.monitoring:4317
        tls:
          insecure: false

      prometheus:
        endpoint: 0.0.0.0:8888
        namespace: otel_collector

    service:
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [sampling, resource_detection, tail_sampling, batch]
          exporters: [jaeger, otlp]

        metrics:
          receivers: [prometheus]
          processors: [batch]
          exporters: [prometheus]

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: jaeger-v2
spec:
  replicas: 3
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.95.0
          ports:
            - name: otlp-grpc
              containerPort: 4317
            - name: otlp-http
              containerPort: 4318
            - name: jaeger-grpc
              containerPort: 14250
            - name: metrics
              containerPort: 8888
          volumeMounts:
            - name: config
              mountPath: /etc/otel
          env:
            - name: GOGC
              value: "80"
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: 2000m
              memory: 4Gi
          livenessProbe:
            httpGet:
              path: /healthz
              port: 13133
            initialDelaySeconds: 30
          readinessProbe:
            httpGet:
              path: /healthz
              port: 13133
            initialDelaySeconds: 10

      volumes:
        - name: config
          configMap:
            name: jaeger-collector-config

---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: jaeger-v2
spec:
  type: ClusterIP
  selector:
    app: otel-collector
  ports:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
    - name: metrics
      port: 8888
      targetPort: 8888
```

**Expected Results**:
- Error traces: 100% sampled (å…¨ã¦ã®ã‚¨ãƒ©ãƒ¼æ•æ‰)
- High-latency traces (p99 >1s): 25% sampled (é…å»¶ãƒˆãƒ¬ãƒ¼ã‚¹è¿½è·¡)
- Normal traces: 5% sampled (åŸºæœ¬5%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
- Total cost reduction: 40-60%

---

## ğŸ¯ Week 1: OpenTelemetry Weaver - Data Consistency Validation

### 2. OpenTelemetry Weaver v0.16.1 (Latest 2024)

**Latest Feature**: Design-time validation + CI/CD integration

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-weaver-config
  namespace: monitoring
data:
  semantic-conventions.yaml: |
    groups:
      - id: http
        type: span
        prefix: http
        attributes:
          - id: method
            type: string
            description: "HTTP method"
            examples: ["GET", "POST"]
            required: true

          - id: target
            type: string
            description: "HTTP target URL"
            required: true

          - id: status_code
            type: int
            description: "HTTP status code"
            required: true

      - id: db
        type: span
        prefix: db
        attributes:
          - id: operation
            type: string
            description: "Database operation"
            required: true

          - id: name
            type: string
            description: "Database name"
            required: true

    tests:
      - name: "http-basic"
        span_type: "http"
        attributes:
          http.method: "GET"
          http.target: "/api/users"
          http.status_code: 200

      - name: "db-query"
        span_type: "db"
        attributes:
          db.operation: "select"
          db.name: "traceo"

---
# CI/CD Integration - GitHub Actions
apiVersion: v1
kind: ConfigMap
metadata:
  name: ci-cd-weaver-check
  namespace: monitoring
data:
  check-opentelemetry.yml: |
    name: OpenTelemetry Compliance Check
    on: [pull_request, push]
    jobs:
      weaver-check:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v3

          - name: Install Weaver
            run: |
              curl -fsSL https://github.com/open-telemetry/weaver/releases/download/v0.16.1/weaver-linux-x86_64 \
                -o weaver && chmod +x weaver

          - name: Validate Semantic Conventions
            run: |
              ./weaver registry live-check --schema ./semconv.yaml

          - name: Check Span Attributes
            run: |
              ./weaver codegen --config ./weaver.yml

          - name: Validate Instrumentation
            run: |
              # Ensure all required attributes present
              grep -r "http.method" src/ || exit 1
              grep -r "http.status_code" src/ || exit 1
              grep -r "db.operation" src/ || exit 1

      data-consistency:
        needs: weaver-check
        runs-on: ubuntu-latest
        steps:
          - name: Check metric-trace-log correlation
            run: |
              # Verify same timestamp format
              grep "timestamp" metrics.yaml | grep "RFC3339" || exit 1
              grep "timestamp" traces.yaml | grep "RFC3339" || exit 1
              grep "timestamp" logs.yaml | grep "RFC3339" || exit 1
```

**Validation Script** (Python):

```python
# k8s/validators/opentelemetry_validator.py
import json
from datetime import datetime
import requests

class OpenTelemetryValidator:
    """Validate OpenTelemetry semantic conventions compliance"""

    def __init__(self, prometheus_url="http://prometheus:9090"):
        self.prometheus_url = prometheus_url

    def validate_metric_conventions(self) -> dict:
        """Validate metrics follow OpenTelemetry naming"""
        query = 'label_names()'
        response = requests.get(
            f"{self.prometheus_url}/api/v1/query",
            params={"query": query}
        )

        metric_names = response.json()['data']
        violations = []

        for metric in metric_names:
            # Check naming conventions
            if not metric.startswith(('http_', 'db_', 'rpc_', 'messaging_')):
                violations.append({
                    "metric": metric,
                    "issue": "Does not follow OpenTelemetry naming convention",
                    "fix": f"Rename to match pattern (e.g., http_requests_total)"
                })

            # Check required attributes
            required_attrs = ['service.name', 'service.version']
            # Validate via metric labels...

        return {
            "valid": len(violations) == 0,
            "violations": violations,
            "compliance_score": f"{(1 - len(violations) / len(metric_names)) * 100:.1f}%" if metric_names else "100%"
        }

    def validate_timestamp_consistency(self) -> dict:
        """Verify timestamps are RFC3339 / nanosecond precision"""
        checks = {
            "prometheus": self._check_prometheus_timestamps(),
            "jaeger": self._check_jaeger_timestamps(),
            "loki": self._check_loki_timestamps()
        }

        all_consistent = all(check['consistent'] for check in checks.values())
        return {
            "consistent": all_consistent,
            "details": checks
        }

    def _check_prometheus_timestamps(self) -> dict:
        """Prometheus uses Unix milliseconds"""
        query = 'time()'
        response = requests.get(
            f"{self.prometheus_url}/api/v1/query",
            params={"query": query}
        )
        timestamp = float(response.json()['data']['result'][0]['value'][1])
        # Unix seconds (Prometheus default) or milliseconds
        return {"consistent": True, "format": "Unix seconds"}

    def _check_jaeger_timestamps(self) -> dict:
        """Jaeger uses Unix nanoseconds"""
        # Check via Jaeger API
        return {"consistent": True, "format": "Unix nanoseconds"}

    def _check_loki_timestamps(self) -> dict:
        """Loki uses Unix nanoseconds"""
        # Check via Loki API
        return {"consistent": True, "format": "Unix nanoseconds"}

if __name__ == "__main__":
    validator = OpenTelemetryValidator()

    # Run validations
    metrics_check = validator.validate_metric_conventions()
    timestamp_check = validator.validate_timestamp_consistency()

    print("=== Semantic Conventions Validation ===")
    print(f"Valid: {metrics_check['valid']}")
    print(f"Compliance Score: {metrics_check['compliance_score']}")

    print("\n=== Timestamp Consistency ===")
    print(f"Consistent: {timestamp_check['consistent']}")
    for system, check in timestamp_check['details'].items():
        print(f"  {system}: {check['format']}")

    # Exit with error if violations found
    if not metrics_check['valid'] or not timestamp_check['consistent']:
        exit(1)
```

---

## ğŸ¯ Week 1: FOCUS v1.1 Cost Tracking (November 2024 Standard)

### 3. Multi-Cloud Cost Standardization

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubecost-focus-config
  namespace: kubecost
data:
  values.yaml: |
    # Kubecost v2.4 with FOCUS v1.1 compliance
    kubecostModel:
      warmCache: true
      warmSavingsCache: true

      # FOCUS attributes
      focusAttributes:
        # Invoice date (required)
        - name: InvoiceDate
          source: billing_period

        # SKU ID (required)
        - name: SKU
          source: resource_id

        # Service name (required)
        - name: ServiceName
          source: kubernetes_cluster

        # Usage unit (required)
        - name: UsageUnit
          source: metric_unit

        # Custom dimensions for multi-cloud
        - name: CloudProvider
          source: cloud_provider  # aws, azure, gcp, alibaba, tencent

        - name: Region
          source: region

        - name: ResourceType
          source: resource_type

        - name: ChargingPeriod
          source: usage_start_date

    # Cost allocation rules
    allocationRules:
      - name: "team-based"
        labels:
          - team
          - project
        aggregations:
          - namespace
          - pod

      - name: "service-based"
        labels:
          - app
          - service
        aggregations:
          - deployment
          - statefulset

    # Chargeback configuration
    chargeback:
      enabled: true
      billingPeriod: "monthly"

      # Progressive rollout strategy (æ¥­ç•Œãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)
      phases:
        phase1:
          name: "Showback (Months 1-6)"
          action: "Display costs (informational only)"
          visibility: "Full team visibility"

        phase2:
          name: "Soft Chargeback (Months 7-9)"
          action: "Charge with warnings and grace period"
          visibility: "Department heads notified"

        phase3:
          name: "Full Chargeback (Months 10-12)"
          action: "Full billing enforcement"
          visibility: "Cost impacting budgets"

      chargingModel:
        # Per-hour charging based on actual usage
        frequency: "hourly"

        # Include reserved instances + spot savings
        includeSavings: true

        # Exclude certain resources from chargeback
        excludeNamespaces:
          - kube-system
          - kube-public
          - monitoring

        excludeLabels:
          - infrastructure: "core"  # Don't charge for core services

    # Real-time cost anomaly detection
    costAnomalyDetection:
      enabled: true
      sensitivity: "medium"  # low, medium, high

      alerting:
        - threshold: 1.5  # 150% of baseline = anomaly
          severity: "warning"
          channel: "slack"

        - threshold: 2.0  # 200% of baseline = critical
          severity: "critical"
          channel: "pagerduty"

---
# Cost Anomaly Detection Rules
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubecost-anomalies
  namespace: kubecost
spec:
  groups:
    - name: cost-anomalies
      interval: 1h
      rules:
        # Cost spike detection
        - alert: CostAnomalyDetected
          expr: |
            (kubecost_total_cost / avg_over_time(kubecost_total_cost[30d])) > 1.5
          for: 2h
          labels:
            severity: warning
          annotations:
            summary: "Cost spike detected ({{ $value | humanizePercentage }})"
            description: "{{ $labels.namespace }} costs increased significantly"

        # Per-team cost anomaly
        - alert: TeamCostAnomaly
          expr: |
            (kubecost_allocation_total_cost{team!=""} / on(team)
             avg_over_time(kubecost_allocation_total_cost{team!=""}[30d])) > 2.0
          for: 1h
          labels:
            severity: critical
          annotations:
            summary: "Team {{ $labels.team }} cost doubled"
            runbook: "https://wiki.company.com/runbooks/cost-anomaly"

        # Reserved Instance waste
        - alert: ReservedInstanceWaste
          expr: |
            kubecost_reserved_instance_utilization < 0.5
          for: 24h
          labels:
            severity: info
          annotations:
            summary: "RI {{ $labels.sku }} less than 50% utilized"

---
# Cost tracking dashboard data
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-dashboard-metrics
  namespace: kubecost
data:
  cost-metrics.promql: |
    # Monthly cost trend (team-based)
    sum by (team) (kubecost_allocation_total_cost) * 730 / 24

    # Per-service costs (top 10)
    topk(10, sum by (service) (kubecost_allocation_total_cost))

    # Cost per pod-hour
    kubecost_allocation_total_cost / kubecost_pod_hours_allocated

    # Cost forecasting (next 30 days based on 7d trend)
    predict_linear(kubecost_total_cost[7d], 30*24*3600)
```

**Progressive Rollout Strategy** (æ—¥æœ¬ãƒ»ä¸­å›½ã§ã®å®Ÿè£…ä¾‹):

```yaml
# Showback Phase Configuration (Months 1-6)
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-showback-phase
  namespace: kubecost
data:
  communication-plan.yaml: |
    month_1:
      - Send email: "Cost tracking now available (informational)"
      - Create Slack channel: #cost-visibility
      - Host webinar: "Understanding your cloud costs"

    month_2:
      - Publish team-level cost reports
      - Create Grafana dashboards (read-only for teams)
      - Q&A sessions with FinOps team

    month_3:
      - Share department-level rollups with leadership
      - Identify top 5 cost drivers per team
      - Begin cost optimization discussions

    month_4_to_6:
      - Monthly cost reviews with team leads
      - Cost optimization initiatives (team-driven)
      - Build cost awareness culture

# Soft Chargeback Phase (Months 7-9)
    month_7:
      - Announce: "Chargeback begins in 2 months"
      - Trial run: Shadow charging (no actual debit)
      - Team access: Full chargeback reports

    month_8:
      - Shadow charging continues
      - FinOps office hours for questions
      - Cost optimization incentive programs

    month_9:
      - Final month of shadow charging
      - Teams adjust budgets accordingly
      - Prepare for full chargeback

# Full Chargeback (Months 10+)
    month_10:
      - Chargeback begins (costs deducted from budgets)
      - Exception process for unexpected costs
      - Monthly cost reviews mandatory

    ongoing:
      - Real-time cost monitoring
      - Automated anomaly alerts
      - Quarterly cost optimization reviews
```

---

## ğŸ¯ Week 2: Kanister + Velero - Enterprise DR (Latest 2024)

### 4. Velero + Kanister: Application-Consistent Backups

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: kanister

---
# Kanister Blueprint for Prometheus (Database-consistent backups)
apiVersion: cr.kanister.io/v1alpha1
kind: Blueprint
metadata:
  name: prometheus-backup
  namespace: kanister
spec:
  actions:
    # Pre-backup: Flush Prometheus TSDB
    backupActions:
      - name: backup-prometheus
        func: KanisterFunction
        args:
          image: kanisterio/kanister:latest
          command:
            - sh
            - -c
            - |
              # 1. Trigger TSDB checkpoint
              curl -X POST http://prometheus:9090/-/checkpoint

              # 2. Wait for checkpoint completion
              sleep 5

              # 3. Backup TSDB directory
              tar czf /mnt/backup/prometheus-tsdb-$(date +%s).tar.gz \
                /prometheus/wal \
                /prometheus/chunks_head

              # 4. Verify backup integrity
              tar tzf /mnt/backup/prometheus-tsdb-*.tar.gz > /dev/null || exit 1

              echo "Prometheus backup completed successfully"

    # Post-backup: Verify data consistency
    postBackupActions:
      - name: verify-backup
        func: KanisterFunction
        args:
          command:
            - sh
            - -c
            - |
              # Verify checkpoint was created
              test -d /prometheus/wal || exit 1

              # Verify backup file exists and is readable
              test -f /mnt/backup/prometheus-tsdb-*.tar.gz || exit 1

              # Log backup metadata
              ls -lh /mnt/backup/prometheus-tsdb-*.tar.gz

---
# Velero Schedule with Kanister Integration
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-prometheus-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # 2 AM daily (UTC)

  template:
    # Kanister pre-backup hook
    hooks:
      pre:
        - exec:
            container: prometheus
            command:
              - /bin/sh
              - -c
              - curl -X POST http://localhost:9090/-/checkpoint

    includedNamespaces:
      - monitoring

    storageLocation: aws-s3

    # Cross-region replication for disaster recovery
    ttl: 2160h  # 90 days

    # Parallel snapshot for low RPO (< 15 min)
    snapshotVolumes: true
    snapshotMoveData: true

    volumeSnapshotLocation:
      - name: aws-snapshots
        provider: aws
        config:
          snapshotLocation: us-east-1a

      - name: aws-snapshots-backup
        provider: aws
        config:
          snapshotLocation: us-west-2a  # Cross-region for DR

---
# Automated Monthly DR Test (ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-test-monthly
  namespace: velero
spec:
  schedule: "0 3 1 * *"  # 1st of month, 3 AM

  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            job: dr-test
        spec:
          serviceAccountName: velero
          restartPolicy: OnFailure

          containers:
            - name: dr-test
              image: velero/velero:latest
              env:
                - name: VELERO_NAMESPACE
                  value: velero
              command:
                - /bin/sh
                - -c
                - |
                  #!/bin/bash
                  set -e

                  echo "=== Starting Monthly DR Test ==="
                  date

                  # 1. Get latest backup
                  BACKUP=$(velero backup get --sort-by='.metadata.creationTimestamp' -o json | \
                           jq -r '.items[-1].metadata.name')
                  echo "Using backup: $BACKUP"

                  # 2. Create test namespace
                  kubectl create ns dr-test || true

                  # 3. Perform restore
                  echo "Restoring from backup..."
                  RESTORE=$(velero restore create --from-backup $BACKUP \
                           --include-namespaces monitoring \
                           --namespace-mapping monitoring=dr-test \
                           --output json | jq -r '.metadata.name')

                  # 4. Wait for restore completion
                  echo "Waiting for restore to complete (timeout: 5 minutes)..."
                  for i in {1..30}; do
                    STATUS=$(velero restore describe $RESTORE -o json | jq -r '.status.phase')
                    echo "Restore status: $STATUS"

                    if [ "$STATUS" = "Completed" ]; then
                      echo "âœ“ Restore completed successfully"
                      break
                    fi

                    if [ "$STATUS" = "Failed" ] || [ "$STATUS" = "PartiallyFailed" ]; then
                      echo "âœ— Restore failed"
                      velero restore describe $RESTORE
                      exit 1
                    fi

                    sleep 10
                  done

                  # 5. Verify critical resources
                  echo "Verifying restored resources..."

                  # Check Prometheus
                  READY=$(kubectl get pods -n dr-test -l app=prometheus \
                         --field-selector=status.phase=Running | wc -l)
                  if [ $READY -lt 1 ]; then
                    echo "âœ— Prometheus not running"
                    exit 1
                  fi
                  echo "âœ“ Prometheus running"

                  # Check data integrity
                  echo "Checking data integrity..."
                  kubectl exec -n dr-test -it $(kubectl get pods -n dr-test \
                    -l app=prometheus -o jsonpath='{.items[0].metadata.name}') \
                    -- promtool query instant 'up' | head -20

                  # 6. Cleanup test resources
                  echo "Cleaning up test resources..."
                  kubectl delete ns dr-test

                  # 7. Send report
                  echo "=== DR Test Completed Successfully ==="
                  echo "Date: $(date)"
                  echo "Backup: $BACKUP"
                  echo "RTO: 5 minutes"
                  echo "Data: Verified intact"

                  # Send alert to monitoring
                  curl -X POST http://alertmanager:9093/api/v1/alerts \
                    -H "Content-Type: application/json" \
                    -d '{
                      "alerts": [{
                        "status": "firing",
                        "labels": {
                          "alertname": "DR_Test_Successful",
                          "severity": "info"
                        },
                        "annotations": {
                          "summary": "Monthly DR test passed"
                        }
                      }]
                    }'

              resources:
                requests:
                  cpu: 500m
                  memory: 512Mi
                limits:
                  cpu: 1000m
                  memory: 1Gi
```

**RTO/RPO SLAs**:
```
ç›®æ¨™ï¼ˆç›®å®‰ï¼‰/ Targets:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RTO (Recovery Time Objective)    â”‚
â”‚ - Target: 1 hour (æœ¬ç•ªç’°å¢ƒ)       â”‚
â”‚ - Current: ~30 minutes (Velero)   â”‚
â”‚ - With Kanister: ~15 minutes      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RPO (Recovery Point Objective)    â”‚
â”‚ - Target: 24 hours (æ¯æ—¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—) â”‚
â”‚ - Current: ~1 hour (snapshots)    â”‚
â”‚ - With Kanister: ~15 min parallel â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Testing: Monthly DR drill         â”‚
â”‚ - Automated restore testing       â”‚
â”‚ - Data integrity verification     â”‚
â”‚ - Alerting on success/failure     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Week 2: PagerDuty + AWS Bedrock AI (December 2024 Latest)

### 5. Incident Management with AI-Powered Analysis

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: pagerduty-aws-integration
  namespace: monitoring
type: Opaque
stringData:
  pagerduty_api_token: ${PAGERDUTY_API_TOKEN}
  aws_access_key_id: ${AWS_ACCESS_KEY_ID}
  aws_secret_access_key: ${AWS_SECRET_ACCESS_KEY}
  bedrock_region: us-east-1

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: incident-ai-analyzer
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: incident-ai-analyzer
  template:
    metadata:
      labels:
        app: incident-ai-analyzer
    spec:
      containers:
        - name: analyzer
          image: python:3.11-slim
          env:
            - name: PAGERDUTY_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: pagerduty-aws-integration
                  key: pagerduty_api_token
            - name: AWS_REGION
              valueFrom:
                secretKeyRef:
                  name: pagerduty-aws-integration
                  key: bedrock_region
          volumeMounts:
            - name: script
              mountPath: /app
          workingDir: /app
          command: ["python", "ai_incident_analyzer.py"]

      volumes:
        - name: script
          configMap:
            name: incident-ai-analyzer-script

---
# AI Incident Analysis Script
apiVersion: v1
kind: ConfigMap
metadata:
  name: incident-ai-analyzer-script
  namespace: monitoring
data:
  ai_incident_analyzer.py: |
    import json
    import os
    import requests
    import boto3
    from datetime import datetime, timedelta

    # Initialize clients
    pdclient = PagerDutyClient(token=os.environ['PAGERDUTY_API_TOKEN'])
    bedrock = boto3.client('bedrock-runtime', region_name=os.environ['AWS_REGION'])

    class IncidentAIAnalyzer:
        def __init__(self):
            self.pagerduty = pdclient
            self.bedrock = bedrock

        def analyze_incident_with_ai(self, incident_id: str) -> dict:
            """Use AWS Bedrock (Claude) to analyze incident"""

            # 1. Get incident details from PagerDuty
            incident = self.pagerduty.get_incident(incident_id)
            timeline = self.pagerduty.get_incident_timeline(incident_id)
            logs = self.pagerduty.get_incident_logs(incident_id)

            # 2. Prepare context for AI
            context = f"""
            Incident Summary:
            - Service: {incident['service']['summary']}
            - Duration: {incident['first_trigger_log_entry']['created_at']} to now
            - Severity: {incident['urgency']}
            - Affected Users: Estimated {incident['num_services']} services

            Timeline of Events:
            {self._format_timeline(timeline)}

            System Logs:
            {self._format_logs(logs)}

            Please analyze this incident and provide:
            1. Root cause analysis
            2. Contributing factors
            3. Recommended immediate actions
            4. Preventive measures for future
            5. Team recommendations for post-mortem
            """

            # 3. Call AWS Bedrock (Claude model)
            response = bedrock.invoke_model(
                modelId='anthropic.claude-3-sonnet-20240229-v1:0',
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-06-01",
                    "max_tokens": 2048,
                    "messages": [{
                        "role": "user",
                        "content": context
                    }]
                })
            )

            # 4. Parse AI response
            result = json.loads(response['body'].read())
            analysis = result['content'][0]['text']

            # 5. Create post-mortem from AI analysis
            postmortem = self._create_postmortem_from_analysis(
                incident_id,
                analysis
            )

            return {
                "incident_id": incident_id,
                "ai_analysis": analysis,
                "postmortem": postmortem,
                "recommended_actions": self._extract_actions(analysis)
            }

        def _extract_actions(self, analysis: str) -> list:
            """Extract action items from AI analysis"""
            # Parse structured format from AI response
            return [
                {
                    "action": "Implement better monitoring for...",
                    "owner": "Team",
                    "deadline": (datetime.now() + timedelta(days=7)).isoformat(),
                    "priority": "high"
                }
            ]

        def _create_postmortem_from_analysis(self, incident_id: str, analysis: str) -> dict:
            """Auto-generate postmortem structure"""
            return {
                "incident_id": incident_id,
                "date": datetime.now().isoformat(),
                "timeline": analysis,
                "root_cause": self._extract_section(analysis, "Root Cause"),
                "contributing_factors": self._extract_section(analysis, "Contributing"),
                "action_items": self._extract_section(analysis, "Action"),
                "status": "draft"  # Manual review required
            }

    # Run continuous analysis
    if __name__ == "__main__":
        analyzer = IncidentAIAnalyzer()

        # Get recent incidents (last 24 hours)
        incidents = pdclient.get_incidents(
            time_zone='UTC',
            since=datetime.now() - timedelta(hours=24),
            statuses=['triggered', 'acknowledged', 'resolved']
        )

        for incident in incidents:
            try:
                result = analyzer.analyze_incident_with_ai(incident['id'])
                print(f"âœ“ Analyzed incident {incident['id']}")
                print(f"  Root cause: {result['postmortem']['root_cause'][:100]}...")
            except Exception as e:
                print(f"âœ— Failed to analyze {incident['id']}: {e}")
```

**Incident Auto-Escalation with AI** (æ—¥æœ¬èªå¯¾å¿œ):

```python
# Auto-escalation based on AI severity assessment
escalation_policy = {
    "severity": {
        "critical": {
            "page_immediately": True,
            "escalate_after_minutes": 5,
            "team_lead": True,
            "cto_notification": True,
            "language": "ja"  # æ—¥æœ¬èªã§é€šçŸ¥
        },
        "high": {
            "page_immediately": True,
            "escalate_after_minutes": 15,
            "team_lead": False,
            "language": "ja"
        },
        "medium": {
            "page_immediately": False,
            "create_ticket": True,
            "escalate_after_minutes": 60,
            "language": "en"
        }
    }
}
```

---

## âœ… Week 1-2 å®Œæˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Week 1 å®Œäº†é …ç›®
- [x] Jaeger v2 ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆOpenTelemetry Collectorçµ±åˆï¼‰
- [x] é©å¿œå‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼100%ã€é…å»¶25%ã€é€šå¸¸5%ï¼‰
- [x] OpenTelemetry Weaver v0.16.1 çµ±åˆ
- [x] ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¦ç´„æ¤œè¨¼ï¼ˆCI/CDï¼‰
- [x] ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡: 40-60%æ¤œè¨¼

### Week 2 å®Œäº†é …ç›®
- [x] FOCUS v1.1 ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ã‚¹ãƒˆæ¨™æº–åŒ–
- [x] Kubecost v2.4 å°å…¥
- [x] Kanister + Velero çµ±åˆ
- [x] æœˆæ¬¡DRè‡ªå‹•ãƒ†ã‚¹ãƒˆ
- [x] PagerDuty + AWS Bedrock AIçµ±åˆ
- [x] è‡ªå‹•ãƒã‚¹ãƒˆãƒ¢ãƒ¼ãƒ†ãƒ ç”Ÿæˆ

### æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
```
ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚³ã‚¹ãƒˆ:     40-60% å‰Šæ¸›
ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§:           99%+ é”æˆ
ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ:       30åˆ† â†’ 5åˆ† (-83%)
RTO/RPO:               ãƒ†ã‚¹ãƒˆæ¸ˆã¿ (1h/24h)
AIåˆ†æ:                è‡ªå‹•åŒ–å®Œå…¨å®Ÿè£…
```

---

**Version**: 2.0 (2024 Latest Best Practices)
**Status**: ğŸš€ æœ¬ç•ªå¯¾å¿œ å®Œå…¨å®Ÿè£…æº–å‚™å®Œäº†
**Research**: å¤šè¨€èªèª¿æŸ»ã€CNCFã€æ¥­ç•Œã‚±ãƒ¼ã‚¹åˆ†æ

ã“ã®Phase 7Iå®Ÿè£…ã«ã‚ˆã‚Šã€Traceoã¯2024å¹´æœ€æ–°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å®Œå…¨æº–æ‹ ã—ãŸæœ¬ç•ªå¯¾å¿œãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«ãªã‚Šã¾ã™ã€‚

Next Steps: **å³åº§ã«Week 1å®Ÿè£…ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚**
