---
# Phase 7N Production Hardening Infrastructure
# Kubernetes manifests for load testing, data pipelines, caching, tracing, and logging
# Date: November 21, 2024

apiVersion: v1
kind: Namespace
metadata:
  name: traceo-7n

---
# ConfigMap for Load Testing Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: k6-load-test-config
  namespace: traceo-7n
data:
  baseline-metrics.json: |
    {
      "main": {
        "latency_p95": 500,
        "latency_p99": 1000,
        "error_rate": 0.01,
        "throughput_rps": 100,
        "timestamp": "2024-11-21T00:00:00Z"
      }
    }
  regression-threshold.json: |
    {
      "latency_regression_percent": 10,
      "error_rate_regression_percent": 10,
      "throughput_degradation_percent": 10
    }

---
# K6 Load Testing Cronjob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: k6-continuous-load-test
  namespace: traceo-7n
spec:
  schedule: "0 */4 * * *"  # Every 4 hours
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: k6-load-tester
          containers:
          - name: k6
            image: grafana/k6:latest
            env:
            - name: TARGET_URL
              value: "http://traceo-api:8000"
            - name: K6_VUS
              value: "50"
            - name: K6_DURATION
              value: "300s"
            - name: K6_CLOUD
              value: "true"
            - name: K6_CLOUD_TOKEN
              valueFrom:
                secretKeyRef:
                  name: k6-secrets
                  key: cloud-token
            volumeMounts:
            - name: test-scripts
              mountPath: /scripts
            args:
            - "run"
            - "/scripts/api-health-test.js"
            - "-o"
            - "cloud"
          restartPolicy: OnFailure
          volumes:
          - name: test-scripts
            configMap:
              name: k6-test-scripts
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 5

---
# Service Account for K6
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k6-load-tester
  namespace: traceo-7n

---
# Secrets for Grafana Cloud
apiVersion: v1
kind: Secret
metadata:
  name: k6-secrets
  namespace: traceo-7n
type: Opaque
data:
  cloud-token: {{ .K6CloudToken | b64enc }}

---
# Kafka Cluster for CDC Pipeline
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
  namespace: traceo-7n
data:
  server.properties: |
    broker.id=1
    listeners=PLAINTEXT://kafka:9092
    advertised.listeners=PLAINTEXT://kafka:9092
    log.dirs=/var/lib/kafka/data
    log.retention.hours=168
    log.segment.bytes=1073741824
    offsets.topic.replication.factor=3
    transaction.state.log.replication.factor=3
    auto.create.topics.enable=true

---
# Zookeeper StatefulSet for Kafka
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper
  namespace: traceo-7n
spec:
  serviceName: zookeeper
  replicas: 3
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
      - name: zookeeper
        image: confluentinc/cp-zookeeper:7.5.0
        env:
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"
        - name: ZOOKEEPER_TICK_TIME
          value: "2000"
        - name: ZOOKEEPER_SYNC_LIMIT
          value: "5"
        - name: ZOOKEEPER_INIT_LIMIT
          value: "10"
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        volumeMounts:
        - name: data
          mountPath: /var/lib/zookeeper/data
        - name: datalog
          mountPath: /var/lib/zookeeper/log
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - zookeeper
            topologyKey: kubernetes.io/hostname
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
  - metadata:
      name: datalog
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi

---
# Zookeeper Service
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: traceo-7n
spec:
  clusterIP: None
  selector:
    app: zookeeper
  ports:
  - port: 2181
    targetPort: 2181
    name: client
  - port: 2888
    targetPort: 2888
    name: server
  - port: 3888
    targetPort: 3888
    name: leader-election

---
# Kafka Broker StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: traceo-7n
spec:
  serviceName: kafka
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.5.0
        env:
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['kafka-broker-id']
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "zookeeper:2181"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://kafka-$(KAFKA_BROKER_ID).kafka:9092"
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_LOG_RETENTION_HOURS
          value: "168"
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"
        ports:
        - containerPort: 9092
          name: broker
        volumeMounts:
        - name: data
          mountPath: /var/lib/kafka/data
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - kafka
            topologyKey: kubernetes.io/hostname
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 100Gi

---
# Kafka Service
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: traceo-7n
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
  - port: 9092
    targetPort: 9092
    name: broker

---
# Debezium Connect Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: debezium-connect
  namespace: traceo-7n
spec:
  replicas: 2
  selector:
    matchLabels:
      app: debezium-connect
  template:
    metadata:
      labels:
        app: debezium-connect
    spec:
      containers:
      - name: debezium
        image: debezium/connect:2.4
        env:
        - name: BOOTSTRAP_SERVERS
          value: "kafka:9092"
        - name: GROUP_ID
          value: "debezium-group"
        - name: CONFIG_STORAGE_TOPIC
          value: "connect_configs"
        - name: OFFSET_STORAGE_TOPIC
          value: "connect_offsets"
        - name: STATUS_STORAGE_TOPIC
          value: "connect_status"
        - name: CONFIG_STORAGE_REPLICATION_FACTOR
          value: "2"
        - name: OFFSET_STORAGE_REPLICATION_FACTOR
          value: "2"
        - name: STATUS_STORAGE_REPLICATION_FACTOR
          value: "2"
        ports:
        - containerPort: 8083
          name: api
        livenessProbe:
          httpGet:
            path: /
            port: 8083
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 8083
          initialDelaySeconds: 20
          periodSeconds: 5
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi

---
# Debezium Service
apiVersion: v1
kind: Service
metadata:
  name: debezium-connect
  namespace: traceo-7n
spec:
  selector:
    app: debezium-connect
  ports:
  - port: 8083
    targetPort: 8083
  type: ClusterIP

---
# Jaeger Deployment for Distributed Tracing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: traceo-7n
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:latest
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        ports:
        - containerPort: 6831
          protocol: UDP
          name: jaeger-agent
        - containerPort: 16686
          name: jaeger-ui
        - containerPort: 14250
          name: grpc
        - containerPort: 14268
          name: http
        - containerPort: 9411
          name: zipkin
        livenessProbe:
          httpGet:
            path: /
            port: 16686
          initialDelaySeconds: 30
          periodSeconds: 10
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi

---
# Jaeger Service
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: traceo-7n
spec:
  selector:
    app: jaeger
  ports:
  - port: 6831
    targetPort: 6831
    protocol: UDP
    name: jaeger-agent
  - port: 16686
    targetPort: 16686
    name: jaeger-ui
  - port: 14250
    targetPort: 14250
    name: grpc
  - port: 14268
    targetPort: 14268
    name: http
  - port: 9411
    targetPort: 9411
    name: zipkin
  type: LoadBalancer

---
# Loki Deployment for Log Aggregation
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: traceo-7n
spec:
  replicas: 2
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
      - name: loki
        image: grafana/loki:latest
        ports:
        - containerPort: 3100
          name: loki
        volumeMounts:
        - name: config
          mountPath: /etc/loki
        - name: data
          mountPath: /loki
        livenessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 20
          periodSeconds: 5
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
      volumes:
      - name: config
        configMap:
          name: loki-config
      - name: data
        emptyDir: {}

---
# Loki Service
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: traceo-7n
spec:
  selector:
    app: loki
  ports:
  - port: 3100
    targetPort: 3100
  type: ClusterIP

---
# Fluent Bit DaemonSet for Log Collection
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: traceo-7n
spec:
  selector:
    matchLabels:
      app: fluent-bit
  template:
    metadata:
      labels:
        app: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:latest
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config
          mountPath: /fluent-bit/etc
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluent-bit-config

---
# Fluent Bit ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: traceo-7n

---
# Fluent Bit ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit
rules:
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]

---
# Fluent Bit ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: traceo-7n

---
# Loki Config
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: traceo-7n
data:
  loki-config.yaml: |
    auth_enabled: false

    ingester:
      chunk_idle_period: 3m
      max_chunk_age: 1h
      max_streams_per_user: 10000

    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h

    schema_config:
      configs:
      - from: 2020-10-24
        store: boltdb-shipper
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h

    server:
      http_listen_port: 3100
      log_level: info

    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/boltdb-shipper-active
        cache_location: /loki/boltdb-shipper-cache
      filesystem:
        directory: /loki/chunks

    chunk_store_config:
      max_look_back_period: 0s

    table_manager:
      retention_deletes_enabled: false
      retention_period: 0s

---
# Fluent Bit Config
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: traceo-7n
data:
  fluent-bit.conf: |
    [SERVICE]
        Daemon Off
        Flush 1
        Log_Level info
        Parsers_File parsers.conf
        Parsers_File custom_parsers.conf

    [INPUT]
        Name tail
        Parser json
        Tag kube.*
        Path /var/log/containers/*.log
        Exclude_Path /var/log/containers/*proxy*.log
        Refresh_Interval 10
        Mem_Buf_Limit 50MB
        Skip_Long_Lines On

    [FILTER]
        Name kubernetes
        Match kube.*
        Kube_URL https://kubernetes.default.svc:443
        Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix kube.var.log.containers.
        Merge_Log On
        Keep_Log Off

    [OUTPUT]
        Name loki
        Match *
        url http://loki:3100/loki/api/v1/push
        labels job=fluentbit
        auto_kubernetes_labels on

---
# Self-Healing Controller Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auto-recovery-controller
  namespace: traceo-7n
spec:
  replicas: 2
  selector:
    matchLabels:
      app: auto-recovery-controller
  template:
    metadata:
      labels:
        app: auto-recovery-controller
    spec:
      serviceAccountName: auto-recovery
      containers:
      - name: controller
        image: traceo/auto-recovery-controller:latest
        env:
        - name: NAMESPACE
          value: "default"
        - name: METRICS_SERVER
          value: "prometheus:9090"
        - name: LOG_LEVEL
          value: "info"
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi

---
# Auto-Recovery ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: auto-recovery
  namespace: traceo-7n

---
# Auto-Recovery ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: auto-recovery
rules:
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "patch", "update"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "delete"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create"]

---
# Auto-Recovery ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: auto-recovery
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: auto-recovery
subjects:
- kind: ServiceAccount
  name: auto-recovery
  namespace: traceo-7n

---
# NetworkPolicy for Phase 7N Infrastructure
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: traceo-7n-network-policy
  namespace: traceo-7n
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: traceo-7n
  - from:
    - podSelector: {}
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: traceo-7n
  - to:
    - podSelector: {}
  - ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53

---
# PodDisruptionBudget for High Availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: kafka-pdb
  namespace: traceo-7n
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: kafka

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zookeeper-pdb
  namespace: traceo-7n
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: zookeeper

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: loki-pdb
  namespace: traceo-7n
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: loki

---
# ResourceQuota for Namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: traceo-7n-quota
  namespace: traceo-7n
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "40Gi"
    limits.cpu: "40"
    limits.memory: "80Gi"
    pods: "100"
    persistentvolumeclaims: "20"

---
# LimitRange for Pod Resource Limits
apiVersion: v1
kind: LimitRange
metadata:
  name: traceo-7n-limits
  namespace: traceo-7n
spec:
  limits:
  - max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    type: Container
  - max:
      cpu: "4"
      memory: "8Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    type: Pod
